{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output parsing\n",
    "\n",
    "We often use LLMs as text processing engines to convert unstructured text into structured data. In this recipe, we will show how we can use an LLM to extract a structured timeline from a narrative paragraph containing years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "We will use the following text, which is a paragraph on the history of the Baker-Berry Library from [Wikipedia](https://en.wikipedia.org/wiki/Baker-Berry_Library):  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original, historic library building is the Fisher Ames Baker Memorial Library; it opened in 1928 with a collection of 240,000 volumes. The building was designed by Jens Fredrick Larson, modeled after Independence Hall in Philadelphia, and funded by a gift to Dartmouth College by George Fisher Baker in memory of his uncle, Fisher Ames Baker, Dartmouth class of 1859. The facility was expanded in 1941 and 1957–1958 and received its one millionth volume in 1970.\n",
      "\n",
      "In 1992, John Berry and the Baker family donated US $30 million for the construction of a new facility, the Berry Library designed by architect Robert Venturi, adjoining the Baker Library. The new complex, the Baker-Berry Library, opened in 2000 and was completed in 2002.[6] The Dartmouth College libraries presently hold over 2 million volumes in their collections.\n"
     ]
    }
   ],
   "source": [
    "unstructured_text = \"\"\"The original, historic library building is the Fisher Ames Baker Memorial Library; it opened in 1928 with a collection of 240,000 volumes. The building was designed by Jens Fredrick Larson, modeled after Independence Hall in Philadelphia, and funded by a gift to Dartmouth College by George Fisher Baker in memory of his uncle, Fisher Ames Baker, Dartmouth class of 1859. The facility was expanded in 1941 and 1957–1958 and received its one millionth volume in 1970.\n",
    "\n",
    "In 1992, John Berry and the Baker family donated US $30 million for the construction of a new facility, the Berry Library designed by architect Robert Venturi, adjoining the Baker Library. The new complex, the Baker-Berry Library, opened in 2000 and was completed in 2002.[6] The Dartmouth College libraries presently hold over 2 million volumes in their collections.\"\"\"\n",
    "\n",
    "print(unstructured_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the history is written in a narrative style. It mentions various important points in the history of Baker-Berry, but not in a way that they can be easily extracted. There are two major challenges here: \n",
    "- Not all years in the text are actually relevant to the task (_\"class of 1859\"_)\n",
    "- Each year needs a succinct summary of the corresponding event\n",
    "\n",
    "We can solve both of these challenges with an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data extraction\n",
    "\n",
    "The most straight-forward approach is to simply prompt a model to extract the timeline of events from the unstructured text. Let's go ahead and try that first! \n",
    "\n",
    "We will start by using what we have learned in previous recipes about instantiating and invoking a chat model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a succinct timeline of events related to the Library:\n",
      "\n",
      "1. 1928 - Fisher Ames Baker Memorial Library opens with 240,000 volumes.\n",
      "2. 1941 - Library expansion.\n",
      "3. 1957-1958 - Library expansion.\n",
      "4. 1970 - Library receives its one millionth volume.\n",
      "5. 1992 - Donation of $30 million for a new library facility.\n",
      "6. 2000 - Baker-Berry Library complex (new facility) opens.\n",
      "7. 2002 - Completion of the Baker-Berry Library complex.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")\n",
    "\n",
    "prompt = (\n",
    "    \"Extract a succinct timeline of events directly related the Library from the following text: \\n\\n\"\n",
    "    + unstructured_text\n",
    ")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked! Well, kind of. The goal was to extract the data as an _data structure_ (like a Python `list`, or  a `dict`), not just another string. We could now manually parse the response, but if you run the above cell multiple times, you will see that the model may produce different formats of the timeline. This makes es it hard to write code that can reliably and reproducibly extract the data. Let's take it one step further and instruct the model to return the data in a specific format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit output structure\n",
    "\n",
    "Ideally, we want the data to be a `list` of `dict` objects, where each `dict` has two fields: `'year'` and `'event'`. The LLM can't return actual Python objects, it can only ever return a string. But let's instruct our model to format the string in such a way that we can parse it back into an object in Python. [JSON   ](https://en.wikipedia.org/wiki/JSON) is a great format for this, because it is string-based and can be easily parsed by [Python's `json` module](https://docs.python.org/3/library/json.html). Here is how we could modify our prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract a succinct timeline of events directly related the Library from the following text. Return the timeline as a list of dictionaries, where each dictionary has two keys: 'year' and 'event'. Format your output in JSON format. The text:\n",
      "\n",
      "The original, historic library building is the Fisher Ames Baker Memorial Library; it opened in 1928 with a collection of 240,000 volumes. The building was designed by Jens Fredrick Larson, modeled after Independence Hall in Philadelphia, and funded by a gift to Dartmouth College by George Fisher Baker in memory of his uncle, Fisher Ames Baker, Dartmouth class of 1859. The facility was expanded in 1941 and 1957–1958 and received its one millionth volume in 1970.\n",
      "\n",
      "In 1992, John Berry and the Baker family donated US $30 million for the construction of a new facility, the Berry Library designed by architect Robert Venturi, adjoining the Baker Library. The new complex, the Baker-Berry Library, opened in 2000 and was completed in 2002.[6] The Dartmouth College libraries presently hold over 2 million volumes in their collections.\n"
     ]
    }
   ],
   "source": [
    "prompt = (\n",
    "    \"Extract a succinct timeline of events directly related the Library from the following text. Return the timeline as a list of dictionaries, where each dictionary has two keys: 'year' and 'event'. Format your output in JSON format. The text:\\n\\n\"\n",
    "    + unstructured_text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's give it another try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the extracted timeline in JSON format:\n",
      "\n",
      "```\n",
      "[\n",
      "  {\n",
      "    \"year\": 1928,\n",
      "    \"event\": \"The Fisher Ames Baker Memorial Library opened with a collection of 240,000 volumes.\"\n",
      "  },\n",
      "  {\n",
      "    \"year\": 1941,\n",
      "    \"event\": \"The library building was expanded.\"\n",
      "  },\n",
      "  {\n",
      "    \"year\": 1957,\n",
      "    \"event\": \"The library building was expanded again (1957-1958)\"\n",
      "  },\n",
      "  {\n",
      "    \"year\": 1970,\n",
      "    \"event\": \"The library received its one millionth volume.\"\n",
      "  },\n",
      "  {\n",
      "    \"year\": 1992,\n",
      "    \"event\": \"John Berry and the Baker family donated $30 million for the construction of the new library.\"\n",
      "  },\n",
      "  {\n",
      "    \"year\": 2000,\n",
      "    \"event\": \"The new Baker-Berry Library complex opened.\"\n",
      "  },\n",
      "  {\n",
      "    \"year\": 2002,\n",
      "    \"event\": \"The Baker-Berry Library complex was completed.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close! But if you run the cell multiple times, you will see that the model adds additional stuff around the JSON string: Markdown tags for a code block, or sometimes a brief preamble or closing summary. However: Many LLMs, including Llama 3.1, are trained to use the markdown codeblock (everything between the triple backticks ` ``` `) around the actual data. So we could write a parser that first extracts the text between those backticks, and then parses that string as a JSON.\n",
    "\n",
    "Lucky for us, LangChain includes [such a parser](https://api.python.langchain.com/en/latest/output_parsers/langchain_core.output_parsers.json.JsonOutputParser.html) already! Since `langchain_dartmouth` is built on LangChain, we can directly use that component with `ChatDartmouth`!\n",
    "\n",
    "By convention, most components in LangChain use the `invoke` method to \"do their thing\", so here we can fit everything together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year': 1928, 'event': 'Fisher Ames Baker Memorial Library opened with a collection of 240,000 volumes'}\n",
      "{'year': 1941, 'event': 'Library building was expanded'}\n",
      "{'year': 1957, 'event': 'Library building expansion continued'}\n",
      "{'year': 1958, 'event': 'Library building expansion completed'}\n",
      "{'year': 1970, 'event': 'Received its one millionth volume'}\n",
      "{'year': 1992, 'event': 'John Berry and the Baker family donated $30 million for a new library'}\n",
      "{'year': 2000, 'event': 'Baker-Berry Library opened'}\n",
      "{'year': 2002, 'event': 'Baker-Berry Library was completed'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "timeline = parser.invoke(response)\n",
    "\n",
    "for event in timeline:\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are [many more output parsers](https://api.python.langchain.com/en/latest/core_api_reference.html#module-langchain_core.output_parsers) available in LangChain for all sorts of different desired output formats. All of them have the same usage pattern demonstrated above: Instruct the model to return the data in a specific format, then pass the model's response through the parser.\n",
    "\n",
    "If there is a specific format you need that is not already supported by any of the available parsers, you can also write your own by subclassing any of them. Let's say instead of a generic JSON, we wanted to extract a [Pandas](https://pandas.pydata.org/docs/index.html) `DataFrame`. We could create such a parser by subclassing the `JsonOutputParser` and adding an additional step to its `invoke` method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year                                              event\n",
      "0  1928  Fisher Ames Baker Memorial Library opened with...\n",
      "1  1941                          Library building expanded\n",
      "2  1957              Library building expanded (1957-1958)\n",
      "3  1970                      Received one millionth volume\n",
      "4  1992  John Berry and the Baker family donated $30 mi...\n",
      "5  2000                         Baker-Berry Library opened\n",
      "6  2002                      Baker-Berry Library completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataFrameParser(JsonOutputParser):\n",
    "    def invoke(self, text: str) -> pd.DataFrame:\n",
    "        json_data = super().invoke(text)\n",
    "        return pd.DataFrame.from_records(json_data)\n",
    "\n",
    "\n",
    "parser = DataFrameParser()\n",
    "response = llm.invoke(prompt)\n",
    "df = parser.invoke(response)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this recipe, we saw that LLMs are great to extract structured data from unstructured text. Since LLMs can only output strings, output parsers are a great tool to convert the text representation of the structured data into Python objects (like lists, dictionaries, or even data frames) for further processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
