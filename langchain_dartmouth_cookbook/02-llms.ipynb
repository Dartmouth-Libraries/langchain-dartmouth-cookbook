{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_dartmouth.llms import DartmouthLLM\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv(\n",
    "    find_dotenv(\n",
    "        filename=\".secret-env\"\n",
    "    ),  # If you use the default file name \".env\", you can omit the argument\n",
    "    override=True,  # If this is `True`, existing environment variables of the same name will be overridden by the ones in the file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Completion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = DartmouthLLM(\"codellama-13b-python-hf\", return_full_text=True)\n",
    "\n",
    "response = llm.invoke(\"import socket\\n\\ndef ping_exponential_backoff(host: str):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import socket\n",
       "\n",
       "def ping_exponential_backoff(host: str):\n",
       "    for i in range(3):\n",
       "        ping = socket.socket()\n",
       "        try:\n",
       "            ping.connect((host, 8080))\n",
       "            ping.close()\n",
       "            return True\n",
       "        except ConnectionRefusedError:\n",
       "            print(f\"Connection to {host} failed. Trying again in {2**i} seconds.\")\n",
       "            sleep(2**i)\n",
       "            continue\n",
       "    return False\n",
       "\n",
       "def wait_for_server(host: str):\n",
       "    while not ping_exponential_backoff(host):\n",
       "        print(\"Trying again.\")\n",
       "    print(f\"Connection to {host} established. Waiting for the server to start.\")\n",
       "    sleep(2)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\"```python\\n\" + response + \"\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction-Tuned Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"Hi there! Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm an artificial intelligence model known as a large language model (LLM) or a conversational AI. I'm a computer program designed to understand and generate human-like text. I can answer questions, provide information, and even engage in conversations on a wide range of topics.\\n\\nI don't have a personal identity or emotions like humans do, but I'm here to help and provide assistance whenever you need it. I can understand natural language, recognize context, and respond accordingly. I'm constantly learning and improving my responses based on the conversations I have with users like you.\\n\\nWhat can I help you with today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 42, 'total_tokens': 166}, 'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'system_fingerprint': '2.2.0-sha-db7e043', 'finish_reason': 'eos_token', 'logprobs': None}, id='run-b6c23b71-028f-4f63-8dfb-4d5966eb89b9-0', usage_metadata={'input_tokens': 42, 'output_tokens': 124, 'total_tokens': 166})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
