{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import DartmouthLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Completion Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = DartmouthLLM(\"codellama-13b-python-hf\", return_full_text=True)\n",
    "\n",
    "response = llm.invoke(\"import socket\\n\\ndef ping_exponential_backoff(host: str):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import socket\n",
       "\n",
       "def ping_exponential_backoff(host: str):\n",
       "    \"\"\"\n",
       "    Returns a tuple containing:\n",
       "    - the ping status (success/fail)\n",
       "    - the ping latency (if successful, else None)\n",
       "    \"\"\"\n",
       "    timeout = 1000\n",
       "    while True:\n",
       "        try:\n",
       "            # send a ping packet to the host\n",
       "            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
       "            sock.settimeout(timeout / 1000)\n",
       "            sock.connect((host, 80))\n",
       "            return (True, timeout)\n",
       "        except socket.error:\n",
       "            # if the host is unreachable, try again\n",
       "            timeout *= 2\n",
       "        finally:\n",
       "            sock.close()\n",
       "\n",
       "def ping_exponential_backoff_iter(host: str):\n",
       "    \"\"\"\n",
       "    Yields the ping status of the host (fail/success)\n",
       "    \"\"\"\n",
       "    while True:\n",
       "        yield ping_exponential_backoff(host)\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(\"```python\\n\" + response + \"\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction-Tuned Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(\"Hi there! Who are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 42, 'total_tokens': 65}, 'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'system_fingerprint': '2.2.0-sha-db7e043', 'finish_reason': 'eos_token', 'logprobs': None}, id='run-ed094057-1173-4df9-81bb-5726f89c4f41-0', usage_metadata={'input_tokens': 42, 'output_tokens': 23, 'total_tokens': 65})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
