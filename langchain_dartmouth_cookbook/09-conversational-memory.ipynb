{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversational Memory\n",
    "\n",
    "As we mentioned in previous recipes, large language models have no internal state, i.e., they do not retain any conversational context from previous messages. A multi-turn conversation works by passing an increasingly longer prompt to the model that includes all previous messages in addition to the most recent one.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "\n",
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\", temperature=0, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-persistent conversational memory \n",
    "\n",
    " We can think of the conversational memory as the history of all messages that have been passed to and received from the model so far. In [Prompt Basics](06-prompt-basics.ipynb), we saw that we can pass a list of messages to a chat model. We can use this mechanism to create a simple conversational memory system by appending every message (outgoing and incoming) to a list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Ask me a riddle!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I have a riddle for you:\n",
      "\n",
      "I am always coming but never arrive,\n",
      "I have a head, but never hair,\n",
      "I have a bed, but never sleep,\n",
      "I have a mouth, but never speak.\n",
      "\n",
      "What am I?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "first_message = HumanMessage(\"Ask me a riddle!\")\n",
    "conversation = [first_message]\n",
    "\n",
    "first_response = llm.invoke(conversation)\n",
    "conversation.append(first_response)\n",
    "\n",
    "for message in conversation:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Ask me a riddle!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I have a riddle for you:\n",
      "\n",
      "I am always coming but never arrive,\n",
      "I have a head, but never hair,\n",
      "I have a bed, but never sleep,\n",
      "I have a mouth, but never speak.\n",
      "\n",
      "What am I?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Is it a unicorn?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "That's a creative answer, but unfortunately, it's not a unicorn. Unicorns are mythical creatures known for their magical powers and are not typically associated with the characteristics I mentioned in the riddle.\n",
      "\n",
      "Here's a hint: think about a natural feature that flows or moves.\n",
      "\n",
      "Want to take another guess?\n"
     ]
    }
   ],
   "source": [
    "second_message = HumanMessage(\"Is it a unicorn?\")\n",
    "conversation.append(second_message)\n",
    "\n",
    "second_response = llm.invoke(conversation)\n",
    "conversation.append(second_response)\n",
    "\n",
    "for message in conversation:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this technique works for relatively simple scenarios, it's not very elegant and requires quite a bit of code to maintain the history. It also can be potentially problematic when the LLM is part of a chain and we don't want to pass the conversation history as an input to the chain.\n",
    "\n",
    "Instead, the LLM component should keep track of the message history internally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, LangChain offers a way to make that happen. We need two things for this:\n",
    "- a component that keeps track of the message history (replacing the simple list above)\n",
    "- a way for the LLM to interact with this list whenever a new (input or output) message arrives (replacing the list management code we wrote above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep track of the message history, we can use a class called `ChatMessageHistory`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This component works very similarly to the simple list we used above, but is more explicitly designed to be used with messages. For example, here is how we create the history from above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Ask me a riddle!'),\n",
       " AIMessage(content='I have a riddle for you:\\n\\nI am always coming but never arrive,\\nI have a head, but never hair,\\nI have a bed, but never sleep,\\nI have a mouth, but never speak.\\n\\nWhat am I?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 41, 'total_tokens': 89}, 'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'system_fingerprint': '2.2.0-sha-db7e043', 'finish_reason': 'eos_token', 'logprobs': None}, id='run-aee1957a-4aa1-47fd-b73d-d0db118e63bc-0', usage_metadata={'input_tokens': 41, 'output_tokens': 48, 'total_tokens': 89}),\n",
       " HumanMessage(content='Is it a unicorn?'),\n",
       " AIMessage(content=\"That's a creative answer, but unfortunately, it's not a unicorn. Unicorns are mythical creatures known for their magical powers and are not typically associated with the characteristics I mentioned in the riddle.\\n\\nHere's a hint: think about a natural feature that flows or moves.\\n\\nWant to take another guess?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 103, 'total_tokens': 166}, 'model_name': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'system_fingerprint': '2.2.0-sha-db7e043', 'finish_reason': 'eos_token', 'logprobs': None}, id='run-630c176b-f76c-413c-b5a3-e8f6f827d6f4-0', usage_metadata={'input_tokens': 103, 'output_tokens': 63, 'total_tokens': 166})]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_message(first_message)\n",
    "history.add_message(first_response)\n",
    "history.add_message(second_message)\n",
    "history.add_message(second_response)\n",
    "\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make an LLM use a `ChatMessageHistory` object, we need to \"attach\" it to the `ChatDartmouth` component by wrapping them with a class called `RunnableWithMessageHistory`. \n",
    "\n",
    "This class assumes that we want to be able to manage multiple conversation histories, as we would in a chat application. It therefore expects a function that returns a chat message history object given a session id. In this example, we only keep track of a single conversation, so we can just return the same history every time. So we just need to write a very simple dummy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ChatMessageHistory()\n",
    "\n",
    "\n",
    "def get_history(session_id):\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** We have to make sure to instantiate the history outside the function. Otherwise, the message history would not persist between calls to `get_history`!</div>\n",
    "\n",
    "Now we have everything we need to tie it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "llm_with_memory = RunnableWithMessageHistory(\n",
    "    runnable=llm,\n",
    "    get_session_history=get_history,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "**Note:** LangChain calls all components that implement the standard interface of the `invoke` and `stream` methods (and some others) a _runnable_. </div>\n",
    "\n",
    "When we invoke this runnable, we have to specify the session id that will be passed to `get_history` (even though we don't use it here):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I have a key but can't open a lock. \n",
      "I have a face but no eyes, nose, or mouth. \n",
      "I have a head, but never weep. \n",
      "What am I?\n"
     ]
    }
   ],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\n",
    "        \"input\": \"Tell me a riddle!\",\n",
    "    },\n",
    "    config={\"configurable\": {\"session_id\": \"whatever\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You can find me in many places, especially in nature, and I'm often associated with time or a specific part of the day. \n",
      "\n",
      "Also, think about the words I used in the riddle: \"face,\" \"head,\" and \"key.\" They might not be referring to their literal meanings.\n"
     ]
    }
   ],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"Give me a hint\"},\n",
    "    config={\"configurable\": {\"session_id\": \"whatever\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You're flowing in the right direction. \n",
      "\n",
      "Yes, the answer is indeed a river. \n",
      "\n",
      "Here's how each line of the riddle relates to a river:\n",
      "\n",
      "- \"I have a key but can't open a lock\": A river often has a \"key\" or a confluence (where it meets another river) but it can't open a lock.\n",
      "- \"I have a face but no eyes, nose, or mouth\": A river has a surface or \"face\" but it doesn't have facial features.\n",
      "- \"I have a head, but never weep\": A river often has a source or \"head\" but it doesn't cry or weep.\n",
      "\n",
      "Great job figuring out the riddle! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"Is it a river?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"whatever\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the message history object to see that indeed keeps track of all the messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Tell me a riddle!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I have a key but can't open a lock. \n",
      "I have a face but no eyes, nose, or mouth. \n",
      "I have a head, but never weep. \n",
      "What am I?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Give me a hint\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You can find me in many places, especially in nature, and I'm often associated with time or a specific part of the day. \n",
      "\n",
      "Also, think about the words I used in the riddle: \"face,\" \"head,\" and \"key.\" They might not be referring to their literal meanings.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Is it a river?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You're flowing in the right direction. \n",
      "\n",
      "Yes, the answer is indeed a river. \n",
      "\n",
      "Here's how each line of the riddle relates to a river:\n",
      "\n",
      "- \"I have a key but can't open a lock\": A river often has a \"key\" or a confluence (where it meets another river) but it can't open a lock.\n",
      "- \"I have a face but no eyes, nose, or mouth\": A river has a surface or \"face\" but it doesn't have facial features.\n",
      "- \"I have a head, but never weep\": A river often has a source or \"head\" but it doesn't cry or weep.\n",
      "\n",
      "Great job figuring out the riddle! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "for message in history.messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks great, doesn't it?\n",
    "\n",
    "One issue remains, however: Depending on our use case, we might want to persist the message history between runs of the program. Or maybe we want to be able to do something more meaningful with the session id in `get_history`, e.g., manage multiple conversations. In the next section, we will learn about ways to achieve both of those things!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent conversational memory\n",
    "\n",
    "While we could write the message history to disk at the end of every run of our program and read it back in at the start of each run, that would be a bit cumbersome and would require additional boilerplate code. We also might want to consider different options to store the history, like a SQL database.\n",
    "\n",
    "LangChain offers a variety of implementations for the [message history](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.chat_message_histories), built on different services. For example, we can store the history to a SQLite database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import SQLChatMessageHistory\n",
    "\n",
    "DB_NAME = \"chat_history.db\"\n",
    "\n",
    "\n",
    "def get_history(session_id):\n",
    "    return SQLChatMessageHistory(session_id, connection=f\"sqlite:///{DB_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Simon, it's nice to meet you. Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "llm_with_memory = RunnableWithMessageHistory(\n",
    "    runnable=llm,\n",
    "    get_session_history=get_history,\n",
    ")\n",
    "\n",
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"Hi, I am Simon!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"simons_convo\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Alex, what can I help you with today?\n"
     ]
    }
   ],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"Hi, I am Alex!\"},\n",
    "    config={\"configurable\": {\"session_id\": \"alex_convo\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the database like any other SQLite database, e.g. with Python's built-in `sqlite3` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1,\n",
       "  'simons_convo',\n",
       "  '{\"type\": \"human\", \"data\": {\"content\": \"Hi, I am Simon!\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}'),\n",
       " (2,\n",
       "  'simons_convo',\n",
       "  '{\"type\": \"ai\", \"data\": {\"content\": \"Hi Simon, it\\'s nice to meet you. Is there something I can help you with or would you like to chat?\", \"additional_kwargs\": {\"refusal\": null}, \"response_metadata\": {\"token_usage\": {\"completion_tokens\": 26, \"prompt_tokens\": 41, \"total_tokens\": 67}, \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"system_fingerprint\": \"2.2.0-sha-db7e043\", \"finish_reason\": \"eos_token\", \"logprobs\": null}, \"type\": \"ai\", \"name\": null, \"id\": \"run-d8f8dcb0-668b-4cad-b4fa-0994e2ff6aa5-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 41, \"output_tokens\": 26, \"total_tokens\": 67}}}'),\n",
       " (3,\n",
       "  'alex_convo',\n",
       "  '{\"type\": \"human\", \"data\": {\"content\": \"Hi, I am Alex!\", \"additional_kwargs\": {}, \"response_metadata\": {}, \"type\": \"human\", \"name\": null, \"id\": null, \"example\": false}}'),\n",
       " (4,\n",
       "  'alex_convo',\n",
       "  '{\"type\": \"ai\", \"data\": {\"content\": \"Hi Alex, what can I help you with today?\", \"additional_kwargs\": {\"refusal\": null}, \"response_metadata\": {\"token_usage\": {\"completion_tokens\": 12, \"prompt_tokens\": 41, \"total_tokens\": 53}, \"model_name\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\", \"system_fingerprint\": \"2.2.0-sha-db7e043\", \"finish_reason\": \"eos_token\", \"logprobs\": null}, \"type\": \"ai\", \"name\": null, \"id\": \"run-cfd8ccb6-40ad-480a-af4e-7a599146bb28-0\", \"example\": false, \"tool_calls\": [], \"invalid_tool_calls\": [], \"usage_metadata\": {\"input_tokens\": 41, \"output_tokens\": 12, \"total_tokens\": 53}}}')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "con = sqlite3.connect(DB_NAME)\n",
    "cur = con.cursor()\n",
    "\n",
    "# By default, the table name is 'message_store'\n",
    "cur.execute(\"SELECT * FROM message_store;\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the conversations are organized by the session ID, so we can continue to have separate conversations by passing the respective session ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Simon.\n"
     ]
    }
   ],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"What's my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"simons_convo\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Your name is Alex.\n"
     ]
    }
   ],
   "source": [
    "llm_with_memory.invoke(\n",
    "    {\"input\": \"What's my name again?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"alex_convo\"}},\n",
    ").pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the database is stored on disk by default, the history is automatically persisted across multiple runs of the program. If you want to use one of the [other implementations ](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.chat_message_histories)of the chat message history, you only need to change the `get_history` function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "LLMs are stateless and thus require the entire conversation to generate the next turn. We can keep track of the conversation manually by maintaining a list of all outgoing and incoming messages. If we want a more elegant solution that can optionally persist the message history using a variety of backends (e.g., a SQL database), we can use one of [LangChain's implementations of the chat message history](https://api.python.langchain.com/en/latest/community_api_reference.html#module-langchain_community.chat_message_histories)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
