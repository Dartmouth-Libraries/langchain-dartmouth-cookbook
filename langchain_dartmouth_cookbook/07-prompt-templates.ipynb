{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates\n",
    "\n",
    "In previous recipes, a prompt was just a simple Python string. We already encountered a situation, where we need to use a variable in the prompt. For example, let's say we want to create a pun generator that creates a pun based on a general topic. Every time we prompt the model, only the topic part of the prompt will change. So what is an efficient, convenient way to handle this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_dartmouth.llms import ChatDartmouth\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building prompts with basic Python strings\n",
    "\n",
    "As we have done before, we could create a simple string prompt and add the topic to it through string concatenation. First, we define the part of the prompt that does not change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are a pun generator. Your task is to generate a pun based on the following topic: \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we add the missing piece when we prompt the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm ready to compile a pun for you. Here's one:\n",
      "\n",
      "Why do programmers prefer dark mode? Because light attracts bugs.\n",
      "\n",
      "Hope that one didn't crash your day!\n"
     ]
    }
   ],
   "source": [
    "llm = ChatDartmouth(model_name=\"llama-3-1-8b-instruct\")\n",
    "response = llm.invoke(prompt + \"computer programming\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That works, but it is a little clunky. The main issue here is that we have to design the prompt in a way that puts all the variable parts at the end. For short prompts like this one, this might be acceptable. It greatly limits our design space, though, when we are dealing with longer instructions. What if want more than one variable part with a constant part in-between?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates\n",
    "\n",
    "Prompt templates (e.g., the [`PromptTemplate` class](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.prompt.PromptTemplate.html)) are components in the LangChain ecosystem that allow you to define your prompts more flexibly by using placeholders and then filling them with actual values when needed.\n",
    "\n",
    "Let's create the same pun generator as above using a `PromptTemplate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"You are a pun generator. Your task is to generate a pun based on the following topic: {topic}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the special substring `{topic}`! This is how we define a location and a name for a placeholder in the prompt!\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "\n",
    "**Note:**\n",
    "Prompt templates are similar to [Python's f-strings or format strings](https://docs.python.org/3/tutorial/inputoutput.html#formatted-string-literals), but offer some additional convenience when using them with other LangChain components, as we will see in some later recipes.\n",
    "</div>\n",
    "\n",
    "We can fill in the placeholder using the `PromptTemplate` component's `invoke` method to fully specify the prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='You are a pun generator. Your task is to generate a pun based on the following topic: computer science'\n"
     ]
    }
   ],
   "source": [
    "print(prompt.invoke(\"computer science\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass the now complete prompt directly to our LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why not \"Byte-Sized Problem\" for the programmer who's got a small issue to tackle?\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt.invoke(\"computer science\"))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we want to run this repeatedly for different topics, we only need to change the prompt template's argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've got a \"degree\" of puns for you. Here's one: \n",
      "\n",
      "\"Why did the college student bring a ladder to class? He wanted to reach his full potential.\"\n",
      "\n",
      "How's that? I'm \"graduating\" to the next pun...\n",
      "----------\n",
      "Here's a soccer pun for you:\n",
      "\n",
      "\"Why did the soccer player bring a pillow onto the field? He wanted to have a soft defense!\"\n",
      "\n",
      "Hope that one scored a goal with you!\n",
      "----------\n",
      "I'm egg-static to generate a pun for you. Here's one: \n",
      "\n",
      "\"Why was the pizza in a bad mood? It was feeling a little crusty!\"\n",
      "\n",
      "Hope that one rises to the occasion!\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "topics = [\"college\", \"soccer\", \"cooking\"]\n",
    "\n",
    "for topic in topics:\n",
    "    response = llm.invoke(prompt.invoke(topic))\n",
    "    print(response.content)\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also extend this technique to multiple placeholders. Here is what the prompt template would look like in that case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"You are a pun generator. Your task is to generate a pun based on the following topic: {topic}. Your current mood is {mood}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have more than placeholder, we cannot simply pass a single argument to the `invoke` method, though, because the prompt would not know which placeholder to map it to. Instead, we pass in a dictionary, using the placeholder names as keys and the desired text to fill-in as values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='You are a pun generator. Your task is to generate a pun based on the following topic: computer science. Your current mood is exhilirated.'\n"
     ]
    }
   ],
   "source": [
    "placeholder_fillers = {\"topic\": \"computer science\", \"mood\": \"exhilirated\"}\n",
    "print(prompt.invoke(placeholder_fillers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can iterate through two lists t of topics and moods to generate pun for each pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*giggles* Okay, let's get this degree of puns going! \n",
      "\n",
      "Here's one: Why did the college student bring a ladder to class? Because they wanted to reach their full potential!\n",
      "----------\n",
      "*Sigh* Oh, the agony and the ecstasy of crafting a pun worthy of the beautiful game. Very well, I shall attempt to score a goal with my words. Here's a soccer pun, dripping with drama:\n",
      "\n",
      "\"Why did the soccer player bring a pillow onto the field? He wanted a soft defense, and to have a ball-anced attack, but ultimately, it was a match that left him feeling deflated...\"\n",
      "\n",
      "*Takes a deep breath* There, I've unleashed my pun upon the world. May it bring a smile to the face of the soccer enthusiast, or may it leave them feeling like they've just been tackled to the ground...\n",
      "----------\n",
      "What a \"saucy\" task you've given me. Here's a pun that's the \"icing on the cake\":\n",
      "\n",
      "Why did the chef quit his job? He couldn't \"whisk\" his responsibilities away â€“ he was \"buttering\" under the pressure!\n",
      "\n",
      "I hope that one \"stirs\" up a smile.\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "moods = [\"giggly\", \"dramatic\", \"whimsical\"]\n",
    "\n",
    "for topic, mood in zip(topics, moods):\n",
    "    response = llm.invoke(prompt.invoke({\"topic\": topic, \"mood\": mood}))\n",
    "    print(response.content)\n",
    "    print(\"-\" * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `PromptTemplate` objects are more than just strings, they have [a few methods and fields](https://api.python.langchain.com/en/latest/core/prompts/langchain_core.prompts.prompt.PromptTemplate.html#langchain_core.prompts.prompt.PromptTemplate) that can be useful in the right circumstances. For example, you can learn the names of the required placeholders using the field `input_variables`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mood', 'topic']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat prompt templates\n",
    "\n",
    "You can also create and use templates for chat prompts with a sequence of messages of different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a dog.'), HumanMessage(content='Tell us about your day.')])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [(\"system\", \"You are a {animal}.\"), (\"human\", \"Tell us about {topic}.\")]\n",
    ")\n",
    "\n",
    "prompt.invoke({\"animal\": \"dog\", \"topic\": \"your day\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My day was so much fun! I woke up early this morning and ran straight to my favorite spot on the couch. My humans always sit there in the mornings, and I love to snuggle up close to them. I gave them a big ol' lick on the face, and they laughed and gave me belly rubs.\n",
      "\n",
      "After that, we went for a walk! Oh boy, I LOVE walks. I got to sniff all the interesting smells and say hello to all the other dogs we met. I'm a bit of a social butterfly (or should I say, social dog?). My tail was wagging the whole time, and I even got to chase a squirrel or two (they're so fast, but it's fun to try to catch them!).\n",
      "\n",
      "When we got back home, my humans gave me a yummy breakfast of kibble and some scrambled eggs. I gobbled it all up in like two seconds. I'm a bit of a foodie, you know. After breakfast, we played a game of fetch in the backyard. I love chasing after that ball and bringing it back to my humans. They always give me such great praise and belly rubs when I do.\n",
      "\n",
      "Now I'm just relaxing on my dog bed, feeling happy and content after a great day. I think I might take a nap soon. My humans are watching TV on the couch, and I'm keeping a close eye on them to make sure they don't get too comfortable without giving me some pets and snuggles.\n",
      "\n",
      "Overall, it's been a paw-some day, and I'm so grateful for my wonderful humans who take care of me and give me so much love and attention. Woof woof!\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(prompt.invoke({\"animal\": \"dog\", \"topic\": \"your day\"}))\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Prompt templates allow us to create a consistent structure for our prompts and make them more re-usable across different applications or tasks. This makes it easier to generate the right kind of input for an AI model, while also making the code cleaner and more readable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
